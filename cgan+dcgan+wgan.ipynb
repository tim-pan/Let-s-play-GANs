{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU State: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from evaluator import evaluation_model as pre_cla\n",
    "import copy\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import dataset\n",
    "from dataset import *\n",
    "from tqdm import tqdm \n",
    "import torch.nn.utils.spectral_norm as spectral_norm \n",
    "import torch\n",
    "\n",
    "\n",
    "'''\n",
    "this model implement cgan+dcgan+wgan, that is:\n",
    "https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html\n",
    "'''\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_classes, img_size, z_dim, upsample_block_num, c_dim=256):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.init_size = self.img_size // 4  #16\n",
    "        \n",
    "        self.z_dim=z_dim\n",
    "        self.c_dim=c_dim\n",
    "        self.latent_dim=self.z_dim+self.c_dim\n",
    "        \n",
    "        self.conditionExpand=nn.Sequential(\n",
    "            nn.Linear(24, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.l1 = nn.Sequential(nn.Linear(self.latent_dim, 128 * self.init_size ** 2))\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        labels = self.conditionExpand(labels.float())#[bs, 256]\n",
    "        z = torch.cat((noise, labels), -1)#(bs, feature_dim(z_dim + n_classes))\n",
    "        \n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_classes, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.n_classes = n_classes\n",
    "        self.convert_label_layer = nn.Sequential(\n",
    "            nn.Linear(self.n_classes, self.img_size**2),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(4, 128, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1024, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(1024, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        c = self.convert_label_layer(labels.float()).view(-1, 1, self.img_size, self.img_size)#(bs, 1, imgsize, imgsize)\n",
    "        out = torch.cat((img, c), 1)#concatenate img tensor and c(label) tensor\n",
    "        \n",
    "        return self.main(out)\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_z(batch_size, z_dim):\n",
    "    return torch.randn(batch_size, z_dim, device=device)\n",
    "\n",
    "def save_img(images, path):\n",
    "    out = make_grid(images)\n",
    "    save_image(out, path)\n",
    "def smooth_label(mode, shape):\n",
    "    \n",
    "    if mode == 'real':\n",
    "        return (torch.rand(shape, device=device) / 10) + 0.9\n",
    "    elif mode == 'fake':\n",
    "        return torch.rand(shape, device=device) / 10\n",
    "\n",
    "    \n",
    "def train(G, D, epochs,lr_g, lr_d, train_loader, test_loader):\n",
    "    adversarial_criterion=nn.BCELoss().to(device)\n",
    "#     generator_criterion = GeneratorLoss().to(device)\n",
    "    \n",
    "    total_loss_g = 0\n",
    "    total_loss_d = 0\n",
    "    num = 0\n",
    "    best_score = 0\n",
    "    \n",
    "    test_label = next(iter(test_loader)).to(device)#[bs, 24]\n",
    "    test_z = random_z(test_label.size(0), z_dim).to(device)\n",
    "    \n",
    "    G.train().to(device)\n",
    "    D.train().to(device)\n",
    "    \n",
    "    G.apply(weights_init_normal)\n",
    "    D.apply(weights_init_normal)\n",
    "    \n",
    "    optimizer_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.9))\n",
    "    optimizer_d = optim.Adam(D.parameters(), lr=lr_d, betas=(0.5, 0.9))\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, epochs):\n",
    "        num=0\n",
    "        \n",
    "        for image, label in tqdm(train_loader):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            num += label.size(0)\n",
    "            real = image.detach()\n",
    "            \n",
    "            ############################\n",
    "            #   Update D network: maximize D(x)-1-D(G(z))  \n",
    "            ############################\n",
    "                \n",
    "            fake_img = G(random_z(label.size(0), z_dim), label).detach()\n",
    "            \n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            loss_d = -torch.mean(D(real, label)) + torch.mean(D(fake_img, label))\n",
    "            \n",
    "            total_loss_d += loss_d.item()\n",
    "            \n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            ###########clipping########\n",
    "            for p in D.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "            \n",
    "            ############################\n",
    "            #   Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "            ############################\n",
    "            #generate fake img\n",
    "#             for _ in range(4):\n",
    "            optimizer_g.zero_grad()\n",
    "\n",
    "            fake_img = G(random_z(label.size(0), z_dim), label)\n",
    "            ##\n",
    "            loss_g = -torch.mean(D(fake_img, label))\n",
    "            loss_g.backward()\n",
    "\n",
    "            optimizer_g.step()\n",
    "\n",
    "            total_loss_g += loss_g.item()\n",
    "            \n",
    "        # evaluate    \n",
    "        G.eval()\n",
    "        D.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen_imgs = G(test_z, test_label)\n",
    "        score = pre_cla().eval(gen_imgs, test_label)\n",
    "        gen_imgs = denorm(gen_imgs, device)\n",
    "        \n",
    "        if score >= best_score:\n",
    "            best_score = score\n",
    "            best_model_wts = copy.deepcopy(G.state_dict())\n",
    "            torch.save(best_model_wts, os.path.join('cgan_dcgan_wgan', 'paras', f'epoch{epoch}_score{score:.2f}.pth'))\n",
    "        print(f\"Epoch[{epoch}/{epochs}]\")\n",
    "        total_loss_d /= num\n",
    "        total_loss_g /= num\n",
    "        \n",
    "        print(f'score: {score}')\n",
    "        print(f'generator_loss:{total_loss_g:.8f}')\n",
    "        print(f'discriminator_loss:{total_loss_d:.8f}')\n",
    "        print()\n",
    "        # savefig\n",
    "        save_image(gen_imgs, os.path.join('cgan_dcgan_wgan', 'results', f'epoch{epoch}.png'), nrow=8, normalize=False)\n",
    "               \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/282 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Found 18009 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/282 [00:20<1:33:46, 20.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5be45089ac27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4ee1108cf02c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(G, D, epochs, lr_g, lr_d, train_loader, test_loader)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mtotal_loss_d\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mloss_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0moptimizer_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m###########clipping########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "from dataset import *\n",
    "\n",
    "root_folder = 'data'\n",
    "z_dim = 100\n",
    "n_classes = 24\n",
    "img_size = 64\n",
    "batch_size = 64\n",
    "upsample_block_num = 6\n",
    "epochs = 200\n",
    "lr_d = 0.00005\n",
    "lr_g = 0.00005\n",
    "\n",
    "train_set = ICLEVRLoader(root_folder, mode = 'train')\n",
    "test_set = ICLEVRLoader(root_folder, mode = 'test')\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                         )\n",
    "test_loader = DataLoader(test_set,\n",
    "                          batch_size=64,\n",
    "                          shuffle=False,\n",
    "                         )\n",
    "\n",
    "G = Generator(n_classes, img_size, z_dim, upsample_block_num)\n",
    "D = Discriminator(24, 64)\n",
    "\n",
    "\n",
    "train(G, D, epochs, lr_g, lr_d, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
